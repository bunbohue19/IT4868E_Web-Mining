{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "PWOXKIp9rG0F"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import datasets\n",
        "from torch.utils.data import DataLoader\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "from torch.optim import AdamW\n",
        "from tqdm import tqdm\n",
        "from torch import nn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "co5WLXmqv6_Q"
      },
      "outputs": [],
      "source": [
        "# Triplet Loss class\n",
        "class TripletLoss(nn.Module):\n",
        "    def __init__(self, margin=1.0):\n",
        "        super(TripletLoss, self).__init__()\n",
        "        self.margin = margin\n",
        "\n",
        "    def forward(self, anchor, positive, negative):\n",
        "        distance_positive = (anchor - positive).pow(2).sum(1)\n",
        "        distance_negative = (anchor - negative).pow(2).sum(1)\n",
        "        losses = torch.relu(distance_positive - distance_negative + self.margin)\n",
        "        return losses.mean()\n",
        "\n",
        "# Move batch to device\n",
        "def batch_to_device(batch, device):\n",
        "    return {key: value.to(device) for key, value in batch.items()}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LCj8HWSXwDZE",
        "outputId": "6192f865-7dbf-4483-a5df-d9ca98d4ec79"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Alibaba-NLP/new-impl You can inspect the repository content at https://hf.co/Alibaba-NLP/gte-multilingual-base.\n",
            "You can avoid this prompt in future by passing the argument `trust_remote_code=True`.\n",
            "\n",
            "Do you wish to run the custom code? [y/N] y\n",
            "The repository Alibaba-NLP/gte-multilingual-base references custom code contained in Alibaba-NLP/new-impl which must be executed to correctly load the model. You can inspect the repository content at https://hf.co/Alibaba-NLP/new-impl .\n",
            " You can inspect the repository content at https://hf.co/Alibaba-NLP/gte-multilingual-base.\n",
            "You can avoid this prompt in future by passing the argument `trust_remote_code=True`.\n",
            "\n",
            "Do you wish to run the custom code? [y/N] y\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at Alibaba-NLP/gte-multilingual-base were not used when initializing NewModel: ['classifier.bias', 'classifier.weight']\n",
            "- This IS expected if you are initializing NewModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing NewModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ],
      "source": [
        "# Load model and dataset\n",
        "model_name = \"Alibaba-NLP/gte-multilingual-base\"\n",
        "dataset = datasets.load_dataset(\"jaeyong2/Ja-emb-PreView\")\n",
        "train_dataloader = DataLoader(dataset['train'], batch_size=8, shuffle=True)\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModel.from_pretrained(model_name)\n",
        "triplet_loss = TripletLoss(margin=1.0)\n",
        "\n",
        "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "# Convert model to bfloat16 if supported\n",
        "if torch.cuda.is_available() and torch.cuda.is_bf16_supported():\n",
        "    model = model.to(torch.bfloat16)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W8u91dLMwHcv",
        "outputId": "9bc142ce-ea76-4a26-9c66-3a7300ce5dbe"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 1/3\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Epoch 1:   0%|          | 63/17467 [04:22<20:17:14,  4.20s/it]"
          ]
        }
      ],
      "source": [
        "# Training loop\n",
        "for epoch in range(3):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    count = 0\n",
        "\n",
        "    print(f\"\\nEpoch {epoch + 1}/3\")\n",
        "\n",
        "    for batch in tqdm(train_dataloader, desc=f\"Training Epoch {epoch + 1}\"):\n",
        "        optimizer.zero_grad()\n",
        "        loss = None\n",
        "\n",
        "        for index in range(len(batch[\"context\"])):\n",
        "            # Tokenize inputs\n",
        "            anchor_encodings = tokenizer(\n",
        "                [batch[\"context\"][index]],\n",
        "                truncation=True,\n",
        "                padding=\"max_length\",\n",
        "                max_length=1024,\n",
        "                return_tensors=\"pt\"\n",
        "            )\n",
        "            positive_encodings = tokenizer(\n",
        "                [batch[\"Title\"][index]],\n",
        "                truncation=True,\n",
        "                padding=\"max_length\",\n",
        "                max_length=256,\n",
        "                return_tensors=\"pt\"\n",
        "            )\n",
        "            negative_encodings = tokenizer(\n",
        "                [batch[\"Fake Title\"][index]],\n",
        "                truncation=True,\n",
        "                padding=\"max_length\",\n",
        "                max_length=256,\n",
        "                return_tensors=\"pt\"\n",
        "            )\n",
        "\n",
        "            # Move to device\n",
        "            anchor_encodings = batch_to_device(anchor_encodings, device)\n",
        "            positive_encodings = batch_to_device(positive_encodings, device)\n",
        "            negative_encodings = batch_to_device(negative_encodings, device)\n",
        "\n",
        "            # Get embeddings (CLS token)\n",
        "            anchor_output = model(**anchor_encodings)[0][:, 0, :]\n",
        "            positive_output = model(**positive_encodings)[0][:, 0, :]\n",
        "            negative_output = model(**negative_encodings)[0][:, 0, :]\n",
        "\n",
        "            # Accumulate loss\n",
        "            if loss == None:\n",
        "                loss = triplet_loss(anchor_output, positive_output, negative_output)\n",
        "            else:\n",
        "                loss += triplet_loss(anchor_output, positive_output, negative_output)\n",
        "\n",
        "        # Average loss over batch\n",
        "        loss /= len(batch[\"context\"])\n",
        "\n",
        "        # Backward pass and optimization\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Track statistics\n",
        "        total_loss += loss.item()\n",
        "        count += 1\n",
        "\n",
        "    # Print epoch statistics\n",
        "    avg_loss = total_loss / count\n",
        "    print(f\"Epoch {epoch + 1} - Average Loss: {avg_loss:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Eg1FaUwRwKGG"
      },
      "outputs": [],
      "source": [
        "# Save the fine-tuned model\n",
        "output_dir = \"./fine_tuned_gte_multilingual_japanese\"\n",
        "model.save_pretrained(output_dir)\n",
        "tokenizer.save_pretrained(output_dir)\n",
        "print(f\"\\nModel saved to {output_dir}\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
